#!/usr/bin/env python3
"""
FINAL COMPREHENSIVE AUDIT

Check everything we might have missed:
1. Numerical results consistency
2. Logic errors in strategies
3. Data leakage detection
4. Discoveries validation
5. Granger causality verification
6. Cointegration test validity
7. Bootstrap implementation
8. Cross-validation correctness
9. Results consistency between runs
10. Master findings accuracy
"""
import sys
sys.path.insert(0, '.')

import json
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# ============================================================
# AUDIT FRAMEWORK
# ============================================================

class FinalAuditResult:
    def __init__(self, name: str):
        self.name = name
        self.status = "PENDING"
        self.issues = []
        self.warnings = []
        self.details = {}

    def critical(self, msg: str):
        self.status = "CRITICAL"
        self.issues.append(f"üî¥ {msg}")

    def error(self, msg: str):
        if self.status != "CRITICAL":
            self.status = "ERROR"
        self.issues.append(f"‚ùå {msg}")

    def warn(self, msg: str):
        if self.status not in ["CRITICAL", "ERROR"]:
            self.status = "WARNING"
        self.warnings.append(f"‚ö†Ô∏è {msg}")

    def info(self, msg: str):
        self.details[msg] = True

    def passed(self):
        if self.status == "PENDING":
            self.status = "PASS"


# ============================================================
# AUDIT 1: DATA LEAKAGE DETECTION
# ============================================================

def audit_data_leakage():
    """Check for any form of data leakage."""
    result = FinalAuditResult("Data Leakage Detection")

    try:
        # Check v2 test file for leakage patterns
        v2_path = Path("experiments/test_all_applications_v2.py")
        with open(v2_path, 'r') as f:
            code = f.read()

        # Pattern 1: Future data in features
        leakage_patterns = [
            (r'\.shift\(-[2-9]\d*\)', "Large negative shift (accessing future)"),
            (r'\.iloc\[.*:.*\+', "Forward-looking slice"),
            (r'future.*=.*shift\(-1\)', "Future variable not properly handled"),
        ]

        import re
        for pattern, desc in leakage_patterns:
            matches = re.findall(pattern, code)
            if matches and 'fwd_' not in str(matches):
                result.warn(f"Potential leakage: {desc}")

        # Pattern 2: Check if train data influences test features
        if 'train' in code and 'test' in code:
            # Check if SI is computed on train only
            if 'compute_si_full_data' in code:
                # This is OK - SI computation is sequential
                result.info("SI computed sequentially (no leakage)")
            elif 'compute_si_train_only' in code:
                result.info("SI computed on train only")

        # Pattern 3: Check for proper position lag
        position_uses = code.count('position *')
        position_shifts = code.count('position.shift(1)')
        if position_uses > position_shifts + 5:
            result.warn("Some positions may not be shifted")

        result.passed()

    except Exception as e:
        result.critical(f"Leakage detection failed: {e}")

    return result


# ============================================================
# AUDIT 2: GRANGER CAUSALITY VERIFICATION
# ============================================================

def audit_granger_causality():
    """Verify Granger causality claims are valid."""
    result = FinalAuditResult("Granger Causality Verification")

    try:
        from src.data.loader_v2 import DataLoaderV2, MarketType
        from src.agents.strategies_v2 import get_default_strategies
        from src.competition.niche_population_v2 import NichePopulationV2
        from statsmodels.tsa.stattools import grangercausalitytests

        loader = DataLoaderV2()
        data = loader.load('SPY', MarketType.STOCKS)

        # Compute SI
        strategies = get_default_strategies('daily')
        population = NichePopulationV2(strategies, n_agents_per_strategy=3, frequency='daily')
        population.run(data)
        si = population.compute_si_timeseries(data, window=7)

        # Compute ADX
        def calc_adx(data, period=14):
            high, low, close = data['high'], data['low'], data['close']
            plus_dm = high.diff().clip(lower=0)
            minus_dm = (-low.diff()).clip(lower=0)
            tr = pd.concat([
                high - low,
                abs(high - close.shift()),
                abs(low - close.shift())
            ], axis=1).max(axis=1)
            atr = tr.rolling(period).sum()
            plus_di = 100 * plus_dm.rolling(period).sum() / (atr + 1e-10)
            minus_di = 100 * minus_dm.rolling(period).sum() / (atr + 1e-10)
            dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)
            return dx.rolling(period).mean() / 100

        adx = calc_adx(data)

        # Align data
        common = si.dropna().index.intersection(adx.dropna().index)
        test_data = pd.DataFrame({
            'si': si.loc[common].values,
            'adx': adx.loc[common].values
        })

        # Run Granger test
        try:
            gc_result = grangercausalitytests(test_data[['adx', 'si']], maxlag=5, verbose=False)

            # Check if SI Granger-causes ADX
            min_pval = min(gc_result[lag][0]['ssr_ftest'][1] for lag in range(1, 6))

            if min_pval < 0.05:
                result.info(f"SI Granger-causes ADX (p={min_pval:.4f})")
            else:
                result.warn(f"SI may not Granger-cause ADX (p={min_pval:.4f})")

            result.details['granger_pval'] = float(min_pval)

        except Exception as e:
            result.warn(f"Granger test issue: {e}")

        result.passed()

    except Exception as e:
        result.critical(f"Granger causality audit failed: {e}")

    return result


# ============================================================
# AUDIT 3: COINTEGRATION VERIFICATION
# ============================================================

def audit_cointegration():
    """Verify SI-ADX cointegration claim."""
    result = FinalAuditResult("Cointegration Verification")

    try:
        from src.data.loader_v2 import DataLoaderV2, MarketType
        from src.agents.strategies_v2 import get_default_strategies
        from src.competition.niche_population_v2 import NichePopulationV2
        from statsmodels.tsa.stattools import coint, adfuller

        loader = DataLoaderV2()

        for asset in ['BTCUSDT', 'SPY']:
            market_type = MarketType.CRYPTO if 'BTC' in asset else MarketType.STOCKS
            data = loader.load(asset, market_type)

            # Compute SI
            strategies = get_default_strategies('daily')
            population = NichePopulationV2(strategies, n_agents_per_strategy=3, frequency='daily')
            population.run(data)
            si = population.compute_si_timeseries(data, window=7)

            # Compute ADX (normalized)
            def calc_adx(data, period=14):
                high, low, close = data['high'], data['low'], data['close']
                plus_dm = high.diff().clip(lower=0)
                minus_dm = (-low.diff()).clip(lower=0)
                tr = pd.concat([
                    high - low,
                    abs(high - close.shift()),
                    abs(low - close.shift())
                ], axis=1).max(axis=1)
                atr = tr.rolling(period).sum()
                plus_di = 100 * plus_dm.rolling(period).sum() / (atr + 1e-10)
                minus_di = 100 * minus_dm.rolling(period).sum() / (atr + 1e-10)
                dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)
                return dx.rolling(period).mean() / 100

            adx = calc_adx(data)

            # Align
            common = si.dropna().index.intersection(adx.dropna().index)
            si_vals = si.loc[common].values
            adx_vals = adx.loc[common].values

            # Cointegration test
            try:
                score, pval, _ = coint(si_vals, adx_vals)

                if pval < 0.05:
                    result.info(f"{asset}: SI-ADX cointegrated (p={pval:.4f})")
                else:
                    result.warn(f"{asset}: SI-ADX NOT cointegrated (p={pval:.4f})")

                result.details[f'{asset}_coint_pval'] = float(pval)

            except Exception as e:
                result.warn(f"{asset} cointegration test failed: {e}")

        result.passed()

    except Exception as e:
        result.critical(f"Cointegration audit failed: {e}")

    return result


# ============================================================
# AUDIT 4: BOOTSTRAP IMPLEMENTATION
# ============================================================

def audit_bootstrap():
    """Verify bootstrap CI implementation is correct."""
    result = FinalAuditResult("Bootstrap Implementation")

    try:
        # Generate known distribution
        np.random.seed(42)
        known_returns = np.random.normal(0.001, 0.02, 500)  # Mean 0.1%, std 2%

        # True Sharpe
        true_sharpe = known_returns.mean() / known_returns.std() * np.sqrt(252)

        # Bootstrap implementation from v2
        def bootstrap_ci(returns, n_boot=1000, alpha=0.05):
            sharpes = []
            n = len(returns)
            for _ in range(n_boot):
                sample = np.random.choice(returns, size=n, replace=True)
                sharpes.append(sample.mean() / sample.std() * np.sqrt(252))
            return {
                'mean': np.mean(sharpes),
                'ci_lower': np.percentile(sharpes, 100 * alpha / 2),
                'ci_upper': np.percentile(sharpes, 100 * (1 - alpha / 2)),
            }

        # Run bootstrap
        ci = bootstrap_ci(known_returns)

        # Verify
        if ci['ci_lower'] <= true_sharpe <= ci['ci_upper']:
            result.info(f"True Sharpe ({true_sharpe:.2f}) within CI [{ci['ci_lower']:.2f}, {ci['ci_upper']:.2f}]")
        else:
            result.warn(f"True Sharpe ({true_sharpe:.2f}) outside CI [{ci['ci_lower']:.2f}, {ci['ci_upper']:.2f}]")

        # Check CI width is reasonable
        ci_width = ci['ci_upper'] - ci['ci_lower']
        if ci_width < 0.1:
            result.warn(f"CI too narrow ({ci_width:.2f}) - may be underestimating uncertainty")
        elif ci_width > 2.0:
            result.warn(f"CI too wide ({ci_width:.2f}) - may need more data")

        result.details['ci_width'] = float(ci_width)
        result.details['bootstrap_mean'] = float(ci['mean'])

        result.passed()

    except Exception as e:
        result.critical(f"Bootstrap audit failed: {e}")

    return result


# ============================================================
# AUDIT 5: WALK-FORWARD CORRECTNESS
# ============================================================

def audit_walk_forward():
    """Verify walk-forward validation is implemented correctly."""
    result = FinalAuditResult("Walk-Forward Correctness")

    try:
        v2_path = Path("experiments/test_all_applications_v2.py")
        with open(v2_path, 'r') as f:
            code = f.read()

        # Check for essential elements
        checks = {
            'train_size': 'train_size' in code or 'train_months' in code,
            'test_size': 'test_size' in code or 'test_months' in code,
            'rolling_window': 'start_idx +=' in code or 'rolling' in code.lower(),
            'no_overlap': 'train_end' in code and 'test_end' in code,
            'oos_sharpe': 'oos_sharpe' in code,
        }

        for check, passed in checks.items():
            if passed:
                result.info(f"{check}: present")
            else:
                result.warn(f"{check}: may be missing")

        # Verify no look-ahead in walk-forward
        if 'fit(' in code and 'predict(' in code:
            # Check if fit is called on train, predict on test
            if 'train' in code and 'test' in code:
                result.info("Train/test separation in walk-forward")

        result.passed()

    except Exception as e:
        result.critical(f"Walk-forward audit failed: {e}")

    return result


# ============================================================
# AUDIT 6: RESULTS CONSISTENCY
# ============================================================

def audit_results_consistency():
    """Check if results are consistent across different runs/versions."""
    result = FinalAuditResult("Results Consistency")

    try:
        # Compare v1 and v2 results if both exist
        v1_path = Path("results/application_testing/full_results.json")
        v2_path = Path("results/application_testing_v2/full_results.json")

        if not v1_path.exists() or not v2_path.exists():
            result.warn("Cannot compare - one version missing")
            result.passed()
            return result

        with open(v1_path) as f:
            v1 = json.load(f)
        with open(v2_path) as f:
            v2 = json.load(f)

        # Compare top strategy
        v1_top = v1['summary'][0]['application'] if 'summary' in v1 else None
        v2_top = v2['summary'][0]['strategy'] if 'summary' in v2 else None

        if v1_top and v2_top:
            if 'Regime' in v1_top and 'Regime' in v2_top:
                result.info("Top strategy consistent: Regime Rebalance")
            else:
                result.warn(f"Top strategy changed: v1={v1_top}, v2={v2_top}")

        # Check if v2 has OOS testing
        if 'test_sharpe' in str(v2):
            result.info("V2 has proper OOS testing")
        else:
            result.warn("V2 may lack OOS testing")

        result.passed()

    except Exception as e:
        result.critical(f"Results consistency audit failed: {e}")

    return result


# ============================================================
# AUDIT 7: STRATEGY LOGIC VERIFICATION
# ============================================================

def audit_strategy_logic():
    """Verify strategy implementations are logically correct."""
    result = FinalAuditResult("Strategy Logic")

    try:
        v2_path = Path("experiments/test_all_applications_v2.py")
        with open(v2_path, 'r') as f:
            code = f.read()

        # Check Risk Budgeting logic
        if '0.5 + si_rank * 1.0' in code:
            result.info("Risk Budgeting: Position scales 0.5-1.5 with SI rank")
        else:
            result.warn("Risk Budgeting: Formula may be different")

        # Check Regime Rebalance logic
        if "si_rank > 0.67" in code or "si_rank.shift(1) > 0.67" in code:
            result.info("Regime Rebalance: Uses 67th percentile threshold")

        # Check Factor Timing logic
        if 'momentum_signal' in code and 'meanrev_signal' in code:
            result.info("Factor Timing: Has both momentum and mean-rev signals")

        # Check if all strategies use lagged signals
        strategies_use_lag = code.count('.shift(1)') >= 5
        if strategies_use_lag:
            result.info("Strategies use lagged signals")
        else:
            result.warn("Some strategies may not use lagged signals")

        result.passed()

    except Exception as e:
        result.critical(f"Strategy logic audit failed: {e}")

    return result


# ============================================================
# AUDIT 8: MASTER FINDINGS VERIFICATION
# ============================================================

def audit_master_findings():
    """Verify claims in MASTER_FINDINGS.md are accurate."""
    result = FinalAuditResult("Master Findings Accuracy")

    try:
        findings_path = Path("docs/MASTER_FINDINGS.md")
        if not findings_path.exists():
            result.warn("MASTER_FINDINGS.md not found")
            result.passed()
            return result

        with open(findings_path, 'r') as f:
            content = f.read()

        # Check key claims
        claims = {
            'SI-ADX correlation': 'adx' in content.lower() and 'correlation' in content.lower(),
            'Mean reversion': 'mean reversion' in content.lower() or 'mean-revert' in content.lower(),
            'Regime detection': 'regime' in content.lower(),
            '150 discoveries': '150' in content or '150 discoveries' in content.lower(),
        }

        for claim, found in claims.items():
            if found:
                result.info(f"Claim documented: {claim}")
            else:
                result.warn(f"Claim may be missing: {claim}")

        # Check for caveats/limitations
        if 'limitation' in content.lower() or 'caveat' in content.lower():
            result.info("Limitations documented")
        else:
            result.warn("Limitations may not be documented")

        result.passed()

    except Exception as e:
        result.critical(f"Master findings audit failed: {e}")

    return result


# ============================================================
# AUDIT 9: NUMERICAL STABILITY
# ============================================================

def audit_numerical_stability():
    """Check for numerical stability issues."""
    result = FinalAuditResult("Numerical Stability")

    try:
        v2_path = Path("experiments/test_all_applications_v2.py")
        with open(v2_path, 'r') as f:
            code = f.read()

        # Check for division by zero protection
        div_zero_protection = '+ 1e-10' in code or '+ 1e-8' in code or '.clip(' in code
        if div_zero_protection:
            result.info("Division by zero protection present")
        else:
            result.warn("May lack division by zero protection")

        # Check for NaN handling
        nan_handling = '.dropna()' in code or '.fillna(' in code
        if nan_handling:
            result.info("NaN handling present")
        else:
            result.warn("May lack NaN handling")

        # Check for extreme value handling
        extreme_handling = '.clip(' in code or 'np.clip(' in code
        if extreme_handling:
            result.info("Extreme value clipping present")

        result.passed()

    except Exception as e:
        result.critical(f"Numerical stability audit failed: {e}")

    return result


# ============================================================
# AUDIT 10: REPRODUCIBILITY CHECK
# ============================================================

def audit_reproducibility():
    """Verify results are reproducible."""
    result = FinalAuditResult("Reproducibility")

    try:
        # Check for random seed
        v2_path = Path("experiments/test_all_applications_v2.py")
        with open(v2_path, 'r') as f:
            code = f.read()

        if 'RANDOM_SEED' in code or 'np.random.seed' in code:
            result.info("Random seed set")
        else:
            result.warn("No random seed found")

        # Check for results saving
        if 'json.dump' in code or 'to_json' in code or 'to_csv' in code:
            result.info("Results saving implemented")
        else:
            result.warn("Results may not be saved")

        # Check for git tracking
        git_path = Path(".git")
        if git_path.exists():
            result.info("Git repository present")

        result.passed()

    except Exception as e:
        result.critical(f"Reproducibility audit failed: {e}")

    return result


# ============================================================
# MAIN
# ============================================================

def main():
    print("\n" + "="*80)
    print("  FINAL COMPREHENSIVE AUDIT")
    print("="*80)
    print(f"  Time: {datetime.now().isoformat()}")

    all_results = []

    audits = [
        ("1. Data Leakage Detection", audit_data_leakage),
        ("2. Granger Causality", audit_granger_causality),
        ("3. Cointegration", audit_cointegration),
        ("4. Bootstrap Implementation", audit_bootstrap),
        ("5. Walk-Forward Correctness", audit_walk_forward),
        ("6. Results Consistency", audit_results_consistency),
        ("7. Strategy Logic", audit_strategy_logic),
        ("8. Master Findings", audit_master_findings),
        ("9. Numerical Stability", audit_numerical_stability),
        ("10. Reproducibility", audit_reproducibility),
    ]

    for name, func in audits:
        print(f"\n  {'-'*70}")
        print(f"  {name}")
        print(f"  {'-'*70}")

        result = func()
        all_results.append(result)

        # Print result
        status_icons = {"PASS": "‚úÖ", "WARNING": "‚ö†Ô∏è", "ERROR": "‚ùå", "CRITICAL": "üî¥"}
        icon = status_icons.get(result.status, "‚ùì")
        print(f"  Status: {icon} {result.status}")

        for issue in result.issues:
            print(f"    {issue}")
        for warning in result.warnings[:3]:
            print(f"    {warning}")
        for key, val in list(result.details.items())[:3]:
            if isinstance(val, bool) and val:
                print(f"    ‚úì {key}")
            elif isinstance(val, (int, float)):
                print(f"    ‚Ä¢ {key}: {val:.4f}")

    # Summary
    print("\n" + "="*80)
    print("  AUDIT SUMMARY")
    print("="*80)

    status_counts = {}
    for r in all_results:
        status_counts[r.status] = status_counts.get(r.status, 0) + 1

    for status, count in sorted(status_counts.items()):
        icon = {"PASS": "‚úÖ", "WARNING": "‚ö†Ô∏è", "ERROR": "‚ùå", "CRITICAL": "üî¥"}.get(status, "‚ùì")
        print(f"  {icon} {status}: {count}")

    # Verdict
    print("\n" + "="*80)
    if any(r.status == "CRITICAL" for r in all_results):
        print("  üî¥ VERDICT: CRITICAL ISSUES - MUST FIX")
    elif any(r.status == "ERROR" for r in all_results):
        print("  ‚ùå VERDICT: ERRORS FOUND - SHOULD FIX")
    elif any(r.status == "WARNING" for r in all_results):
        print("  ‚ö†Ô∏è VERDICT: WARNINGS - REVIEW RECOMMENDED")
    else:
        print("  ‚úÖ VERDICT: ALL AUDITS PASSED")
    print("="*80 + "\n")

    # Save
    out_path = Path("results/final_audit/audit_report.json")
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with open(out_path, 'w') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'audits': [{'name': r.name, 'status': r.status,
                       'issues': r.issues, 'warnings': r.warnings,
                       'details': r.details} for r in all_results],
            'summary': status_counts,
        }, f, indent=2)

    print(f"  Report saved: {out_path}")

    return all_results

if __name__ == "__main__":
    main()
